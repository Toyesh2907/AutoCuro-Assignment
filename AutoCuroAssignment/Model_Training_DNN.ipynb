{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ffa573f-5e18-4588-89fa-036ea46cc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime\n",
    "import ast\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,TimeDistributed,Activation,Reshape,Flatten,GRU,Conv2D,MaxPooling2D,BatchNormalization,LSTM,Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fadae909-869a-458f-9b4e-67f7ad026a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('C:/Users/Lenovo/AutoCuroAssignment/tic_tac_toe_records_minimax_Preprocessed_DNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c3faf38-aca2-4b63-bad5-269739ba3e62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>board</th>\n",
       "      <th>decision</th>\n",
       "      <th>Input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OX   O XX</td>\n",
       "      <td>3</td>\n",
       "      <td>[2, 1, 0, 0, 0, 2, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XX XXOOO</td>\n",
       "      <td>9</td>\n",
       "      <td>[1, 1, 0, 1, 1, 2, 2, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X  O</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 0, 0, 2, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O X  OX X</td>\n",
       "      <td>5</td>\n",
       "      <td>[2, 0, 1, 0, 0, 2, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XOXO  OX</td>\n",
       "      <td>9</td>\n",
       "      <td>[1, 2, 1, 2, 0, 0, 2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>XXOXXO O</td>\n",
       "      <td>9</td>\n",
       "      <td>[1, 1, 2, 1, 1, 2, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>XO XOX</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 2, 0, 1, 2, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>XX   O XO</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>X  OX</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 2, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10020 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           board  decision                        Input\n",
       "0      OX   O XX         3  [2, 1, 0, 0, 0, 2, 0, 1, 1]\n",
       "1      XX XXOOO          9  [1, 1, 0, 1, 1, 2, 2, 2, 0]\n",
       "2       X  O             4  [0, 1, 0, 0, 2, 0, 0, 0, 0]\n",
       "3      O X  OX X         5  [2, 0, 1, 0, 0, 2, 1, 0, 1]\n",
       "4      XOXO  OX          9  [1, 2, 1, 2, 0, 0, 2, 1, 0]\n",
       "...          ...       ...                          ...\n",
       "10015                    3  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "10016  XXOXXO O          9  [1, 1, 2, 1, 1, 2, 0, 2, 0]\n",
       "10017  XO XOX            7  [1, 2, 0, 1, 2, 1, 0, 0, 0]\n",
       "10018  XX   O XO         3  [1, 1, 0, 0, 0, 2, 0, 1, 2]\n",
       "10019     X  OX          2  [0, 0, 0, 1, 0, 0, 2, 1, 0]\n",
       "\n",
       "[10020 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9508f3e-d3be-4951-8dfe-ed3ba5be3670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['Input'] = dataset['Input'].apply(ast.literal_eval)\n",
    "X=dataset['Input']\n",
    "y=dataset['decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7987ec6f-b634-4d95-93bd-6e21126862ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=y-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13bb1cf3-3323-4a5e-9c29-12e5611e2417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2401814d-a61e-47f9-86d8-987856523077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4292d4a-8175-40dd-83e3-01efb24e5b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'pandas.core.series.Series'>, <class 'pandas.core.series.Series'>)\n"
     ]
    }
   ],
   "source": [
    "print((type(X_train),\n",
    "type(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3c59fa-2e18-4fe0-89e0-3956785e9b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10660f63-bff9-4619-b1e9-9a7fcd6e5827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2004, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8892c4ad-5b02-4821-a605-1653dcd77b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(9, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acf9f463-ac1d-41d4-853f-ed1d76230eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.1713 - accuracy: 0.2149\n",
      "Epoch 1: val_loss improved from inf to 2.14532, saving model to best_model.h5\n",
      "251/251 [==============================] - 11s 16ms/step - loss: 2.1704 - accuracy: 0.2154 - val_loss: 2.1453 - val_accuracy: 0.2201\n",
      "Epoch 2/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.9857 - accuracy: 0.2781\n",
      "Epoch 2: val_loss improved from 2.14532 to 2.04824, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.9857 - accuracy: 0.2781 - val_loss: 2.0482 - val_accuracy: 0.2410\n",
      "Epoch 3/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.8949 - accuracy: 0.3035\n",
      "Epoch 3: val_loss improved from 2.04824 to 1.79758, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.8947 - accuracy: 0.3035 - val_loss: 1.7976 - val_accuracy: 0.3253\n",
      "Epoch 4/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.8155 - accuracy: 0.3190\n",
      "Epoch 4: val_loss improved from 1.79758 to 1.74733, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.8137 - accuracy: 0.3201 - val_loss: 1.7473 - val_accuracy: 0.3368\n",
      "Epoch 5/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.7490 - accuracy: 0.3308\n",
      "Epoch 5: val_loss improved from 1.74733 to 1.61885, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.7494 - accuracy: 0.3308 - val_loss: 1.6188 - val_accuracy: 0.3493\n",
      "Epoch 6/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.6721 - accuracy: 0.3540\n",
      "Epoch 6: val_loss improved from 1.61885 to 1.55315, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.6717 - accuracy: 0.3544 - val_loss: 1.5532 - val_accuracy: 0.3613\n",
      "Epoch 7/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.6202 - accuracy: 0.3664\n",
      "Epoch 7: val_loss improved from 1.55315 to 1.47586, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.6203 - accuracy: 0.3663 - val_loss: 1.4759 - val_accuracy: 0.3917\n",
      "Epoch 8/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.5585 - accuracy: 0.3819\n",
      "Epoch 8: val_loss improved from 1.47586 to 1.43650, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.5585 - accuracy: 0.3819 - val_loss: 1.4365 - val_accuracy: 0.4137\n",
      "Epoch 9/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.5153 - accuracy: 0.3906\n",
      "Epoch 9: val_loss improved from 1.43650 to 1.39218, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.5164 - accuracy: 0.3897 - val_loss: 1.3922 - val_accuracy: 0.4301\n",
      "Epoch 10/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.4765 - accuracy: 0.4064\n",
      "Epoch 10: val_loss improved from 1.39218 to 1.36255, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.4756 - accuracy: 0.4066 - val_loss: 1.3626 - val_accuracy: 0.4177\n",
      "Epoch 11/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.4391 - accuracy: 0.4194\n",
      "Epoch 11: val_loss improved from 1.36255 to 1.34469, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.4399 - accuracy: 0.4189 - val_loss: 1.3447 - val_accuracy: 0.4147\n",
      "Epoch 12/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.4180 - accuracy: 0.4246\n",
      "Epoch 12: val_loss improved from 1.34469 to 1.31541, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.4183 - accuracy: 0.4244 - val_loss: 1.3154 - val_accuracy: 0.4346\n",
      "Epoch 13/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.3944 - accuracy: 0.4268\n",
      "Epoch 13: val_loss did not improve from 1.31541\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.3947 - accuracy: 0.4265 - val_loss: 1.3177 - val_accuracy: 0.4187\n",
      "Epoch 14/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3833 - accuracy: 0.4253\n",
      "Epoch 14: val_loss improved from 1.31541 to 1.30066, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.3833 - accuracy: 0.4253 - val_loss: 1.3007 - val_accuracy: 0.4376\n",
      "Epoch 15/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.3532 - accuracy: 0.4480\n",
      "Epoch 15: val_loss improved from 1.30066 to 1.28308, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.3537 - accuracy: 0.4477 - val_loss: 1.2831 - val_accuracy: 0.4351\n",
      "Epoch 16/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.3425 - accuracy: 0.4413\n",
      "Epoch 16: val_loss improved from 1.28308 to 1.27004, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.3431 - accuracy: 0.4410 - val_loss: 1.2700 - val_accuracy: 0.4266\n",
      "Epoch 17/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.3252 - accuracy: 0.4509\n",
      "Epoch 17: val_loss improved from 1.27004 to 1.26101, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.3241 - accuracy: 0.4513 - val_loss: 1.2610 - val_accuracy: 0.4386\n",
      "Epoch 18/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.3126 - accuracy: 0.4529\n",
      "Epoch 18: val_loss improved from 1.26101 to 1.24921, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.3131 - accuracy: 0.4532 - val_loss: 1.2492 - val_accuracy: 0.4411\n",
      "Epoch 19/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.3006 - accuracy: 0.4528\n",
      "Epoch 19: val_loss improved from 1.24921 to 1.24247, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.3002 - accuracy: 0.4536 - val_loss: 1.2425 - val_accuracy: 0.4491\n",
      "Epoch 20/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.2896 - accuracy: 0.4659\n",
      "Epoch 20: val_loss improved from 1.24247 to 1.22660, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.2897 - accuracy: 0.4651 - val_loss: 1.2266 - val_accuracy: 0.4516\n",
      "Epoch 21/200\n",
      "246/251 [============================>.] - ETA: 0s - loss: 1.2750 - accuracy: 0.4647\n",
      "Epoch 21: val_loss improved from 1.22660 to 1.21721, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.2760 - accuracy: 0.4646 - val_loss: 1.2172 - val_accuracy: 0.4601\n",
      "Epoch 22/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.2603 - accuracy: 0.4775\n",
      "Epoch 22: val_loss improved from 1.21721 to 1.21644, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.2603 - accuracy: 0.4775 - val_loss: 1.2164 - val_accuracy: 0.4661\n",
      "Epoch 23/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2467 - accuracy: 0.4756\n",
      "Epoch 23: val_loss did not improve from 1.21644\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.2464 - accuracy: 0.4760 - val_loss: 1.2247 - val_accuracy: 0.4661\n",
      "Epoch 24/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.2390 - accuracy: 0.4817\n",
      "Epoch 24: val_loss improved from 1.21644 to 1.19532, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.2390 - accuracy: 0.4817 - val_loss: 1.1953 - val_accuracy: 0.4845\n",
      "Epoch 25/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.2171 - accuracy: 0.4967\n",
      "Epoch 25: val_loss improved from 1.19532 to 1.19342, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.2198 - accuracy: 0.4954 - val_loss: 1.1934 - val_accuracy: 0.4870\n",
      "Epoch 26/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.2077 - accuracy: 0.4975\n",
      "Epoch 26: val_loss improved from 1.19342 to 1.18289, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.2073 - accuracy: 0.4978 - val_loss: 1.1829 - val_accuracy: 0.4915\n",
      "Epoch 27/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.1878 - accuracy: 0.5102\n",
      "Epoch 27: val_loss improved from 1.18289 to 1.17341, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.1876 - accuracy: 0.5106 - val_loss: 1.1734 - val_accuracy: 0.4805\n",
      "Epoch 28/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.1780 - accuracy: 0.5117\n",
      "Epoch 28: val_loss improved from 1.17341 to 1.15670, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.1808 - accuracy: 0.5109 - val_loss: 1.1567 - val_accuracy: 0.5130\n",
      "Epoch 29/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1585 - accuracy: 0.5173\n",
      "Epoch 29: val_loss improved from 1.15670 to 1.14324, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.1586 - accuracy: 0.5172 - val_loss: 1.1432 - val_accuracy: 0.5220\n",
      "Epoch 30/200\n",
      "246/251 [============================>.] - ETA: 0s - loss: 1.1497 - accuracy: 0.5238\n",
      "Epoch 30: val_loss improved from 1.14324 to 1.14221, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.1503 - accuracy: 0.5248 - val_loss: 1.1422 - val_accuracy: 0.5180\n",
      "Epoch 31/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.1348 - accuracy: 0.5318\n",
      "Epoch 31: val_loss improved from 1.14221 to 1.13584, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.1334 - accuracy: 0.5324 - val_loss: 1.1358 - val_accuracy: 0.5274\n",
      "Epoch 32/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1377 - accuracy: 0.5341\n",
      "Epoch 32: val_loss improved from 1.13584 to 1.12500, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.1373 - accuracy: 0.5344 - val_loss: 1.1250 - val_accuracy: 0.5319\n",
      "Epoch 33/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.1143 - accuracy: 0.5424\n",
      "Epoch 33: val_loss did not improve from 1.12500\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.1161 - accuracy: 0.5408 - val_loss: 1.1454 - val_accuracy: 0.5235\n",
      "Epoch 34/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.0966 - accuracy: 0.5466\n",
      "Epoch 34: val_loss did not improve from 1.12500\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0977 - accuracy: 0.5465 - val_loss: 1.1346 - val_accuracy: 0.5324\n",
      "Epoch 35/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.0881 - accuracy: 0.5480\n",
      "Epoch 35: val_loss improved from 1.12500 to 1.11580, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0881 - accuracy: 0.5480 - val_loss: 1.1158 - val_accuracy: 0.5364\n",
      "Epoch 36/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.0666 - accuracy: 0.5638\n",
      "Epoch 36: val_loss did not improve from 1.11580\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0676 - accuracy: 0.5634 - val_loss: 1.1214 - val_accuracy: 0.5314\n",
      "Epoch 37/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.0794 - accuracy: 0.5596\n",
      "Epoch 37: val_loss improved from 1.11580 to 1.11434, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0790 - accuracy: 0.5595 - val_loss: 1.1143 - val_accuracy: 0.5284\n",
      "Epoch 38/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0659 - accuracy: 0.5679\n",
      "Epoch 38: val_loss improved from 1.11434 to 1.10467, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0655 - accuracy: 0.5679 - val_loss: 1.1047 - val_accuracy: 0.5404\n",
      "Epoch 39/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.0469 - accuracy: 0.5655\n",
      "Epoch 39: val_loss did not improve from 1.10467\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.0465 - accuracy: 0.5654 - val_loss: 1.1141 - val_accuracy: 0.5449\n",
      "Epoch 40/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.0565 - accuracy: 0.5728\n",
      "Epoch 40: val_loss improved from 1.10467 to 1.10426, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0548 - accuracy: 0.5741 - val_loss: 1.1043 - val_accuracy: 0.5429\n",
      "Epoch 41/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0294 - accuracy: 0.5790\n",
      "Epoch 41: val_loss did not improve from 1.10426\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0308 - accuracy: 0.5787 - val_loss: 1.1264 - val_accuracy: 0.5364\n",
      "Epoch 42/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.0257 - accuracy: 0.5785\n",
      "Epoch 42: val_loss did not improve from 1.10426\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0268 - accuracy: 0.5780 - val_loss: 1.1077 - val_accuracy: 0.5504\n",
      "Epoch 43/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 1.0088 - accuracy: 0.5830\n",
      "Epoch 43: val_loss did not improve from 1.10426\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0101 - accuracy: 0.5825 - val_loss: 1.1106 - val_accuracy: 0.5514\n",
      "Epoch 44/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 1.0183 - accuracy: 0.5820\n",
      "Epoch 44: val_loss improved from 1.10426 to 1.10380, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.0172 - accuracy: 0.5826 - val_loss: 1.1038 - val_accuracy: 0.5439\n",
      "Epoch 45/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 1.0028 - accuracy: 0.5883\n",
      "Epoch 45: val_loss improved from 1.10380 to 1.09650, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 1.0024 - accuracy: 0.5883 - val_loss: 1.0965 - val_accuracy: 0.5614\n",
      "Epoch 46/200\n",
      "246/251 [============================>.] - ETA: 0s - loss: 1.0035 - accuracy: 0.5920\n",
      "Epoch 46: val_loss improved from 1.09650 to 1.09294, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 1.0030 - accuracy: 0.5913 - val_loss: 1.0929 - val_accuracy: 0.5579\n",
      "Epoch 47/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9818 - accuracy: 0.6022\n",
      "Epoch 47: val_loss improved from 1.09294 to 1.09023, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9818 - accuracy: 0.6023 - val_loss: 1.0902 - val_accuracy: 0.5594\n",
      "Epoch 48/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.9786 - accuracy: 0.5959\n",
      "Epoch 48: val_loss improved from 1.09023 to 1.08784, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9786 - accuracy: 0.5959 - val_loss: 1.0878 - val_accuracy: 0.5599\n",
      "Epoch 49/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 0.9677 - accuracy: 0.6009\n",
      "Epoch 49: val_loss did not improve from 1.08784\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9694 - accuracy: 0.6000 - val_loss: 1.0940 - val_accuracy: 0.5624\n",
      "Epoch 50/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9545 - accuracy: 0.6033\n",
      "Epoch 50: val_loss did not improve from 1.08784\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.9543 - accuracy: 0.6032 - val_loss: 1.1072 - val_accuracy: 0.5624\n",
      "Epoch 51/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.6117\n",
      "Epoch 51: val_loss did not improve from 1.08784\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9611 - accuracy: 0.6117 - val_loss: 1.1033 - val_accuracy: 0.5649\n",
      "Epoch 52/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.9426 - accuracy: 0.6151\n",
      "Epoch 52: val_loss did not improve from 1.08784\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9426 - accuracy: 0.6151 - val_loss: 1.0880 - val_accuracy: 0.5704\n",
      "Epoch 53/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 0.9467 - accuracy: 0.6121\n",
      "Epoch 53: val_loss did not improve from 1.08784\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9470 - accuracy: 0.6117 - val_loss: 1.0986 - val_accuracy: 0.5699\n",
      "Epoch 54/200\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.9335 - accuracy: 0.6160\n",
      "Epoch 54: val_loss improved from 1.08784 to 1.08008, saving model to best_model.h5\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.9335 - accuracy: 0.6160 - val_loss: 1.0801 - val_accuracy: 0.5699\n",
      "Epoch 55/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 0.9276 - accuracy: 0.6174\n",
      "Epoch 55: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9282 - accuracy: 0.6173 - val_loss: 1.1009 - val_accuracy: 0.5609\n",
      "Epoch 56/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9127 - accuracy: 0.6276\n",
      "Epoch 56: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.9121 - accuracy: 0.6279 - val_loss: 1.1028 - val_accuracy: 0.5739\n",
      "Epoch 57/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9073 - accuracy: 0.6305\n",
      "Epoch 57: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9069 - accuracy: 0.6306 - val_loss: 1.0887 - val_accuracy: 0.5749\n",
      "Epoch 58/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 0.9241 - accuracy: 0.6166\n",
      "Epoch 58: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9222 - accuracy: 0.6174 - val_loss: 1.0889 - val_accuracy: 0.5768\n",
      "Epoch 59/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 0.9034 - accuracy: 0.6270\n",
      "Epoch 59: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.9025 - accuracy: 0.6277 - val_loss: 1.1083 - val_accuracy: 0.5768\n",
      "Epoch 60/200\n",
      "247/251 [============================>.] - ETA: 0s - loss: 0.8937 - accuracy: 0.6339\n",
      "Epoch 60: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.8938 - accuracy: 0.6336 - val_loss: 1.1061 - val_accuracy: 0.5808\n",
      "Epoch 61/200\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8848 - accuracy: 0.6428\n",
      "Epoch 61: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.8852 - accuracy: 0.6428 - val_loss: 1.1145 - val_accuracy: 0.5793\n",
      "Epoch 62/200\n",
      "246/251 [============================>.] - ETA: 0s - loss: 0.8870 - accuracy: 0.6302\n",
      "Epoch 62: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.8894 - accuracy: 0.6300 - val_loss: 1.1243 - val_accuracy: 0.5734\n",
      "Epoch 63/200\n",
      "249/251 [============================>.] - ETA: 0s - loss: 0.8775 - accuracy: 0.6353\n",
      "Epoch 63: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.8795 - accuracy: 0.6349 - val_loss: 1.1062 - val_accuracy: 0.5714\n",
      "Epoch 64/200\n",
      "248/251 [============================>.] - ETA: 0s - loss: 0.8773 - accuracy: 0.6368\n",
      "Epoch 64: val_loss did not improve from 1.08008\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.8765 - accuracy: 0.6375 - val_loss: 1.1067 - val_accuracy: 0.5853\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',  \n",
    "    monitor='val_loss',  \n",
    "    save_best_only=True,  \n",
    "    mode='min', \n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=200, \n",
    "    batch_size=32,  \n",
    "    validation_data=(X_test, y_test),  \n",
    "    callbacks=[checkpoint, early_stopping]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf0a2a01-aec9-4e05-9d0a-fe8faef2e3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('C:/Users/Lenovo/AutoCuroAssignment/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4393c23e-f82c-4cfc-a87e-2fc5e32e2a50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f445584a-3b05-4397-a93c-a5997f425045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2798081e-04, 9.7515248e-03, 3.8760909e-01, ..., 3.5219942e-03,\n",
       "        8.1960998e-06, 1.5075077e-05],\n",
       "       [2.7257384e-04, 2.6797945e-06, 1.7829563e-03, ..., 2.9332255e-04,\n",
       "        5.4564855e-05, 9.5071355e-09],\n",
       "       [3.1467743e-02, 7.5092740e-02, 1.1400519e-06, ..., 1.3123500e-01,\n",
       "        4.3613079e-05, 1.3698797e-01],\n",
       "       ...,\n",
       "       [9.7298570e-02, 1.1493265e-01, 1.0948007e-01, ..., 1.1562131e-01,\n",
       "        1.0743031e-01, 1.0253124e-01],\n",
       "       [2.7257437e-04, 2.6797945e-06, 1.7829554e-03, ..., 2.9332284e-04,\n",
       "        5.4564909e-05, 9.5071355e-09],\n",
       "       [2.7257437e-04, 2.6797945e-06, 1.7829554e-03, ..., 2.9332284e-04,\n",
       "        5.4564909e-05, 9.5071355e-09]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29215dab-1ab6-4d8c-ab05-91d53d76376f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [ np.argmax(label) for label in y_pred]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2beef7dc-b765-485a-843d-62d371e28a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.49      0.54       227\n",
      "           2       0.50      0.45      0.48       133\n",
      "           3       0.53      0.63      0.58       245\n",
      "           4       0.48      0.35      0.40       121\n",
      "           5       0.78      0.76      0.77       441\n",
      "           6       0.27      0.68      0.39       152\n",
      "           7       0.66      0.43      0.52       273\n",
      "           8       0.65      0.43      0.52       150\n",
      "           9       0.64      0.59      0.62       262\n",
      "\n",
      "    accuracy                           0.57      2004\n",
      "   macro avg       0.57      0.53      0.53      2004\n",
      "weighted avg       0.61      0.57      0.58      2004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "class_labels=['1','2','3','4','5','6','7','8','9']\n",
    "cr=classification_report(y_test,y_pred,target_names=class_labels)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2bb171e-d752-40a7-9fec-b8bfcc9adbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "model=load_model('C:/Users/Lenovo/AutoCuroAssignment/best_model.h5')\n",
    "data_input=[1,1,0,2,2,0,1,1,0]\n",
    "data_array=np.array(data_input)\n",
    "data_array = np.array(data_array.tolist())\n",
    "data_array = data_array.astype(np.float32)\n",
    "data_array=data_array.reshape(1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07985d33-9c65-42ab-9d6d-8fffc22664b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9acd75d-9be4-4093-980d-2833beeae500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(data_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af651bf6-dbe3-4695-b09f-7283fb40f221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
